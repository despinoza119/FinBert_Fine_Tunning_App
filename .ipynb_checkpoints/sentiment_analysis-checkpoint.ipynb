{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec283a49-a102-433c-ae86-16e8368560d0",
   "metadata": {},
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3faae59-29d0-4276-b30f-06056655fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install pandas\n",
    "#!pip install pyarrow\n",
    "#!pip install numpy\n",
    "#!pip install torch torchvision torchaudio\n",
    "#!pip install transformers\n",
    "#!pip install matplotlib\n",
    "#!pip install nltk\n",
    "#!pip install spacy\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d04969-0f47-4b3d-9adb-a7835c4fb0e7",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68a2256-5ede-42b5-a8ce-a4ed1979e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenization\n",
    "import spacy\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Models\n",
    "# Base Model - Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Base Model - Other Models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Transformers Model\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification,BertTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bc278-1983-4043-afb2-2b6b7a40be00",
   "metadata": {},
   "source": [
    "**Import data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954c376-a28d-4c16-a065-949ad3a78016",
   "metadata": {},
   "source": [
    "- Categorization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7fa4e4-c8c2-4062-a688-40ce882d6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train = pd.read_csv('./categorizacion/topic_train.csv')\n",
    "twitter_test = pd.read_csv('./categorizacion/topic_valid.csv')\n",
    "twitter = pd.concat([twitter_train,twitter_test],axis=0)\n",
    "\n",
    "# with open('./categorizacion/labels.json', 'r') as json_file:\n",
    "#     labels = pd.DataFrame(list(json.load(json_file).items()),columns=['Label','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad262f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here are Thursday's biggest analyst calls: App...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buy Las Vegas Sands as travel to Singapore bui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Piper Sandler downgrades DocuSign to sell, cit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analysts react to Tesla's latest earnings, bre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Netflix and its peers are set for a ‘return to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Here are Thursday's biggest analyst calls: App...      0\n",
       "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
       "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
       "3  Analysts react to Tesla's latest earnings, bre...      0\n",
       "4  Netflix and its peers are set for a ‘return to...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08394704-ccb4-469a-a52a-c500b4884bb8",
   "metadata": {},
   "source": [
    "- Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7bc99b-bb68-4e2d-b125-59b24dee187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_sent='./sentimiento'\n",
    "data_list = []\n",
    "\n",
    "for filename in os.listdir(directory_sent):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_sent,filename)\n",
    "        with open(file_path,'r',encoding='latin1') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                sentence, sentiment = line.rsplit('@',1)\n",
    "                sentiment=sentiment.strip()\n",
    "                data_list.append({'sentence':sentence,'sentiment': sentiment})\n",
    "\n",
    "sent = pd.DataFrame(data_list)\n",
    "sent = sent.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf216f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment\n",
       "0  According to Gran , the company has no plans t...   neutral\n",
       "1  Technopolis plans to develop in stages an area...   neutral\n",
       "2  With the new production plant the company woul...  positive\n",
       "3  According to the company 's updated strategy f...  positive\n",
       "4  For the last quarter of 2010 , Componenta 's n...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c530e67d-e70b-432d-b878-6d53ac8bb9c7",
   "metadata": {},
   "source": [
    "**Pre-processing functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b300e7ac-7b93-4249-a63b-4440f75aa7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stop_words(data):\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    lista = [palabra for palabra in data if palabra not in stop_words and palabra not in string.punctuation and palabra != '``']\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdcf6d4-b4dc-40ea-a4e0-2e8f9bca6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Text\n",
    "sent_base=sent.copy()\n",
    "sent_base['tokenized_sentence'] = sent_base['sentence'].apply(word_tokenize)\n",
    "sent_base['cleaned_sentence']=sent_base['tokenized_sentence'].apply(clean_stop_words)\n",
    "sent_base['processed_sentence'] = sent_base['cleaned_sentence'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Labeling \"sentiment\" (target variable)\n",
    "value_mapping = {'negative': 1, 'neutral': 2, 'positive': 0}\n",
    "sent_base.loc[:, 'sentiment_numeric'] = sent_base['sentiment'].map(value_mapping)\n",
    "\n",
    "# Train-Test Split (80% - 20%)\n",
    "sent_train, sent_test = train_test_split(sent_base, test_size=0.2, stratify=sent_base['sentiment'], random_state=42)\n",
    "\n",
    "# Vectorizing\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# train (sent_train_tfid -> features, y_sent_train -> target)\n",
    "sent_train_tfid = vectorizer.fit_transform(sent_train['processed_sentence'])\n",
    "y_sent_train=sent_train['sentiment_numeric']\n",
    "\n",
    "# test (sent_test_tfid -> features, y_sent_test -> target)\n",
    "sent_test_tfid = vectorizer.transform(sent_test['processed_sentence'])\n",
    "y_sent_test=sent_test['sentiment_numeric']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865cfac-adbd-4dcd-af3a-406e186ab4c1",
   "metadata": {},
   "source": [
    "**Base Line Model (Logistic Regression)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad3242c7-0518-4db9-8b50-0b89880e0e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "-----------------Logistic Regression-----------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.52      0.60       272\n",
      "           1       0.64      0.32      0.43       121\n",
      "           2       0.74      0.91      0.82       575\n",
      "\n",
      "    accuracy                           0.73       968\n",
      "   macro avg       0.69      0.58      0.61       968\n",
      "weighted avg       0.72      0.73      0.71       968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(sent_train_tfid, y_sent_train)\n",
    "\n",
    "predictions = model.predict(sent_test_tfid)\n",
    "\n",
    "accuracy = accuracy_score(y_sent_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(\"-----------------Logistic Regression-----------------\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_sent_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d812eb-d3ad-4681-b3da-c38a2880a150",
   "metadata": {},
   "source": [
    "**Base Line Model (Other Models)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94db01d2-ddd7-42a6-9706-471208336f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression:\n",
      "  Training Accuracy: 0.8083677685950413\n",
      "  Test Accuracy: 0.7262396694214877\n",
      "------\n",
      "RandomForestClassifier:\n",
      "  Training Accuracy: 0.9974173553719008\n",
      "  Test Accuracy: 0.7117768595041323\n",
      "------\n",
      "SVC:\n",
      "  Training Accuracy: 0.918904958677686\n",
      "  Test Accuracy: 0.7200413223140496\n",
      "------\n",
      "MultinomialNB:\n",
      "  Training Accuracy: 0.7551652892561983\n",
      "  Test Accuracy: 0.6859504132231405\n",
      "------\n",
      "GradientBoostingClassifier:\n",
      "  Training Accuracy: 0.8060433884297521\n",
      "  Test Accuracy: 0.7324380165289256\n",
      "------\n",
      "KNeighborsClassifier:\n",
      "  Training Accuracy: 0.7104855371900827\n",
      "  Test Accuracy: 0.6394628099173554\n",
      "------\n",
      "DecisionTreeClassifier:\n",
      "  Training Accuracy: 0.9974173553719008\n",
      "  Test Accuracy: 0.6590909090909091\n",
      "------\n",
      "MLPClassifier:\n",
      "  Training Accuracy: 0.9974173553719008\n",
      "  Test Accuracy: 0.6766528925619835\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faustobravocuvi/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "    MultinomialNB(),\n",
    "    GradientBoostingClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    MLPClassifier()\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(sent_train_tfid, y_sent_train)\n",
    "    \n",
    "    y_train_pred = model.predict(sent_train_tfid)\n",
    "    train_accuracy = accuracy_score(y_sent_train, y_train_pred)\n",
    "    \n",
    "    y_test_pred = model.predict(sent_test_tfid)\n",
    "    test_accuracy = accuracy_score(y_sent_test, y_test_pred)\n",
    "    \n",
    "    print(f\"{model.__class__.__name__}:\")\n",
    "    print(f\"  Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255316b",
   "metadata": {},
   "source": [
    "### FINBERT MODEL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42317eda-bb64-444b-abcd-4b863c402884",
   "metadata": {},
   "source": [
    "**Initiate Model FinBert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf9b28a-0d4c-447a-a073-a455c1465497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47a9d7-9814-4440-af2a-e2d2a538963f",
   "metadata": {},
   "source": [
    "**Calculate Sentiment (FinBert)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e149d85-8a76-44f9-8fe1-77c22c5fba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False,return_tensors = 'pt')\n",
    "    outputs = model(**tokens)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    category = torch.argmax(probabilities).item()\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336e8a1c-61f5-4264-a752-563b57a5f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a copy and encoding categorical labels\n",
    "sent_testing = sent.copy()\n",
    "value_mapping = {'negative': 1, 'neutral': 2, 'positive': 0}\n",
    "sent_testing.loc[:, 'sentiment_numeric'] = sent_testing['sentiment'].map(value_mapping)\n",
    "\n",
    "# Generating a random sample \n",
    "sent_testing_sample = sent_testing.sample(n=4840, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded16ffc-83a4-4045-a7c3-967d8bb2e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to assign sentiment\n",
    "sent_testing_sample['prediction']=sent_testing_sample['sentence'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ec334-eba7-42f2-bc13-8cf53267dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction'])\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction']))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction'])\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733be5ee-1ed4-4a6e-b907-15152d3d2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_testing_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1991cc",
   "metadata": {},
   "source": [
    "### Finbert (Twitter Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# # Evaluate accuracy\n",
    "# accuracy = accuracy_score(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction'])\n",
    "# print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# # Display classification report\n",
    "# print('Classification Report:')\n",
    "# print(classification_report(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction']))\n",
    "\n",
    "# # Display confusion matrix\n",
    "# conf_matrix = confusion_matrix(sent_testing_sample['sentiment_numeric'], sent_testing_sample['prediction'])\n",
    "# print('Confusion Matrix:')\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df21b03",
   "metadata": {},
   "source": [
    "**Calculate Sentiment (FinBert)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False,return_tensors = 'pt')\n",
    "    outputs = model(**tokens)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    category = torch.argmax(probabilities).item()\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df20416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a copy and encoding categorical labels\n",
    "sent_testing = twitter.copy()\n",
    "value_mapping = {'negative': 1, 'neutral': 2, 'positive': 0}\n",
    "# sent_testing.loc[:, 'sentiment_numeric'] = sent_testing['sentiment'].map(value_mapping)\n",
    "\n",
    "# Generating a random sample \n",
    "sent_testing_sample = sent_testing.sample(n=10, random_state=42)\n",
    "\n",
    "# Using the model to assign sentiment\n",
    "sent_testing_sample['prediction']=sent_testing_sample['text'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_testing.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3efb0-b011-4bff-bd8e-288778aa9b44",
   "metadata": {},
   "source": [
    "**Calculate Sentiment (Fine Tuning - FinBert)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05db1d-4a27-41f5-82d5-fc03e6399df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Setting a copy and encoding categorical labels\n",
    "sent_tuning = sent.copy()\n",
    "value_mapping = {'negative': 1, 'neutral': 2, 'positive': 0}\n",
    "sent_tuning.loc[:, 'sentiment_numeric'] = sent_tuning['sentiment'].map(value_mapping)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(sent_tuning['sentence'], sent_tuning['sentiment_numeric'],stratify=sent_tuning['sentiment_numeric'], test_size=0.2,)\n",
    "\n",
    "# Convert the texts and labels into tensors\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128,return_tensors='pt')\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128,return_tensors='pt')\n",
    "\n",
    "# Create attention masks\n",
    "train_masks = train_encodings['attention_mask']\n",
    "val_masks = val_encodings['attention_mask']\n",
    "\n",
    "train_inputs = train_encodings['input_ids']\n",
    "val_inputs = val_encodings['input_ids']\n",
    "\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "val_labels = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# Create the DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4a07c84-fc56-4cc7-8ae6-4ead6e27d153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/daniel/anaconda3/envs/data_driven/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)  # Assuming 3 classes: negative, neutral, positive\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # You can adjust the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {'input_ids': batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device),\n",
    "                  'labels': batch[2].to(device)}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "#model.save_pretrained('fine_tuned_bert_model')\n",
    "\n",
    "# Save the model\n",
    "with open('fine_tuned_bert_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0af9b441-ddab-43ff-b30f-a748540f2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "with open('fine_tuned_bert_model.pkl', 'rb') as f:\n",
    "    model_fine_tuning = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e6a8d29-3a8f-4c2f-8acb-cb4f7cb03211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_tuned(txt):\n",
    "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False,return_tensors = 'pt')\n",
    "    outputs = model_fine_tuning(**tokens)\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    category = torch.argmax(probabilities).item()\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d605f0a0-42e2-4d88-a70b-3c3008bb7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tun = pd.DataFrame({'sentence': val_texts, 'sentiment_numeric': val_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed5877d7-b444-4667-a431-53ac3562c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_tun['prediction']=df_test_tun['sentence'].apply(get_sentiment_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "987a01a4-0352-4c7a-925f-72f2d14975cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.39      0.53       272\n",
      "           1       0.94      0.24      0.38       121\n",
      "           2       0.70      0.98      0.82       575\n",
      "\n",
      "    accuracy                           0.72       968\n",
      "   macro avg       0.81      0.54      0.58       968\n",
      "weighted avg       0.76      0.72      0.68       968\n",
      "\n",
      "Confusion Matrix:\n",
      "[[107   0 165]\n",
      " [ 18  29  74]\n",
      " [  8   2 565]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(df_test_tun['sentiment_numeric'], df_test_tun['prediction'])\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(df_test_tun['sentiment_numeric'], df_test_tun['prediction']))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(df_test_tun['sentiment_numeric'], df_test_tun['prediction'])\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f09efa-a225-4b29-92cb-060f53eb0bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
