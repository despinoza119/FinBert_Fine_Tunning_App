{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial news categorization/sentiment analysis using NLP techniques\n",
    "\n",
    "\n",
    "Sentiment analysis is the statistical analysis of simple sentiment\n",
    "cues. Essentially, it involves making statistical analyses on polarized\n",
    "statements (i.e., statements with a positive, negative and neutral sen\n",
    "timent), which are usually collected in the form of social media posts,\n",
    "reviews, and news articles. Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain.\n",
    "\n",
    "\n",
    "In our case, we will focus on two different tasks.\n",
    "\n",
    "\n",
    "1. **Category tagger**: Create a NLP classifier capable of assigning a financial category to a text derived from the financial industry.\n",
    "\n",
    "The Twitter Financial News dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their topic.\n",
    "\n",
    "    The dataset holds 21,107 documents annotated with 20 labels:\n",
    "\n",
    "topics = {\n",
    "    \"LABEL_0\": \"Analyst Update\",\n",
    "    \"LABEL_1\": \"Fed | Central Banks\",\n",
    "    \"LABEL_2\": \"Company | Product News\",\n",
    "    \"LABEL_3\": \"Treasuries | Corporate Debt\",\n",
    "    \"LABEL_4\": \"Dividend\",\n",
    "    \"LABEL_5\": \"Earnings\",\n",
    "    \"LABEL_6\": \"Energy | Oil\",\n",
    "    \"LABEL_7\": \"Financials\",\n",
    "    \"LABEL_8\": \"Currencies\",\n",
    "    \"LABEL_9\": \"General News | Opinion\",\n",
    "    \"LABEL_10\": \"Gold | Metals | Materials\",\n",
    "    \"LABEL_11\": \"IPO\",\n",
    "    \"LABEL_12\": \"Legal | Regulation\",\n",
    "    \"LABEL_13\": \"M&A | Investments\",\n",
    "    \"LABEL_14\": \"Macro\",\n",
    "    \"LABEL_15\": \"Markets\",\n",
    "    \"LABEL_16\": \"Politics\",\n",
    "    \"LABEL_17\": \"Personnel Change\",\n",
    "    \"LABEL_18\": \"Stock Commentary\",\n",
    "    \"LABEL_19\": \"Stock Movement\"\n",
    "}\n",
    "\n",
    "2. **Sentiment tagger**: Create a NLP classifier capable of assigning a sentiment score (positive,negative,neutral) to text derived from the financial industry. Additionally, we will use a powerful pre-trained model, finetuned on financial data, to assign scores to financial headlines, data from social media posts, etc ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "\n",
    "\n",
    "High level requirements of Python library.\n",
    "\n",
    "    - Pytorch\n",
    "    - HuggingFace Transformers library\n",
    "    - Pandas\n",
    "    - Numpy\n",
    "    - Sklearn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Pulling the data together**\n",
    "\n",
    "\n",
    "Download and inspect the data from the various sources:\n",
    "\n",
    "1. Financial Phrasebank https://huggingface.co/datasets/financial_phrasebank. Humanly annotated\n",
    "\n",
    "2. Financial tweets topics dataset: https://huggingface.co/datasets/zeroshot/twitter-financial-news-topic/viewer/default/train?p=169. Humanly annotated\n",
    "\n",
    "Think of any pre-processing functions (\n",
    "    Converting the text to lowercase,\n",
    "    removing punctuation,\n",
    "    tokenizing the text,\n",
    "    removing stop words and empty strings,\n",
    "    lemmatizing tokens.\n",
    ") that you might need to apply for downstream tasks. As always, pick a framework for data analysis and data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daniel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries import\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets de categorizacion\n",
    "cat_train = pd.read_csv('./categorizacion/topic_train.csv')\n",
    "cat_test = pd.read_csv('./categorizacion/topic_valid.csv')\n",
    "\n",
    "directory_sent='./sentimiento'\n",
    "data_list = []\n",
    "\n",
    "# Datasets de sentimiento\n",
    "for filename in os.listdir(directory_sent):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_sent,filename)\n",
    "\n",
    "        with open(file_path,'r',encoding='latin1') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                sentence, sentiment = line.rsplit('@',1)\n",
    "                sentiment=sentiment.strip()\n",
    "                data_list.append({'sentence':sentence,'sentiment': sentiment})\n",
    "df = pd.DataFrame(data_list)\n",
    "sent_train, sent_test = train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
    "\n",
    "# Output:\n",
    "# cat_train,cat_test,sent_train,sent_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stop_words(data):\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    lista = [palabra for palabra in data if palabra not in stop_words and palabra not in string.punctuation and palabra != '``']\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizacion - Tokenizacion\n",
    "cat_train['tokenized_text']=cat_train['text'].apply(word_tokenize)\n",
    "cat_test['tokenized_text']=cat_test['text'].apply(word_tokenize)\n",
    "\n",
    "# Sentimiento - Tokenizacion\n",
    "sent_train['tokenized_text']=sent_train['sentence'].apply(word_tokenize)\n",
    "sent_test['tokenized_text']=sent_test['sentence'].apply(word_tokenize)\n",
    "\n",
    "# Categorizacion - Stop Words\n",
    "cat_train['cleaned_text']=cat_train['tokenized_text'].apply(clean_stop_words)\n",
    "cat_test['cleaned_text']=cat_test['tokenized_text'].apply(clean_stop_words)\n",
    "\n",
    "# Sentimiento - Stop Words\n",
    "sent_train['cleaned_text']=sent_train['tokenized_text'].apply(clean_stop_words)\n",
    "sent_test['cleaned_text']=sent_test['tokenized_text'].apply(clean_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Train and fine-tune various NLP classifiers on financial news datasets** \n",
    "\n",
    "\n",
    "\n",
    "#### **2.1 Let´s start with simple baseline (at your own choice)**. For example, build a logistic regression model based on pre-trained word embeddings or TF-IDF vectors of the financial news corpus **\n",
    "\n",
    "\n",
    "Build a baseline model  with **Financial Phrasebank dataset**. What are the limitations of these baseline models?\n",
    "\n",
    "\n",
    "#### **2.2 Compare the baseline with a pre-trained model that is specialized for the finance domain. Download and use the FinBERT model from Huggingfaces**\n",
    "\n",
    "Model source: https://huggingface.co/ProsusAI/finbert\n",
    "\n",
    "Once you have downloaded the model, run inference and compute performance metrics to get a sense of how the specialized pre-trained model fares against the baseline  model.  Use the HuggingFaces library to download the model and run inference on it. For large datasets or text sequences, CPU running time might be large.\n",
    "\n",
    "For more information on the model: Araci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models.\n",
    "\n",
    "#### **2.3 (Advanced) Fine-tune a pre-trained model such a base BERT model on a small labeled dataset**\n",
    "\n",
    "General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora.\n",
    "\n",
    "In recent years the NLP community has seen many breakthoughs in Natural Language Processing, especially the shift to transfer learning. Models like ELMo, fast.ai's ULMFiT, Transformer and OpenAI's GPT have allowed researchers to achieves state-of-the-art results on multiple benchmarks and provided the community with large pre-trained models with high performance. This shift in NLP is seen as NLP's ImageNet moment, a shift in computer vision a few year ago when lower layers of deep learning networks with million of parameters trained on a specific task can be reused and fine-tuned for other tasks, rather than training new networks from scratch.\n",
    "\n",
    "One of the most significant milestones in the evolution of NLP recently is the release of Google's BERT, which is described as the beginning of a new era in NLP. In our case, we are going to explore a pre-trained model called FinBERT, already tuned with a financial corpus. I specifically recommend the HuggingFace library for easeness of implementation.\n",
    "\n",
    "*What is HuggingFace?* Hugging Face’s Transformers is an open-source library that provides thousands of pre-trained models to perform various tasks on texts such as text classification, named entity recognition, translation, and more. The library has a unified, high-level API for these models and supports a wide range of languages and model architectures.\n",
    "\n",
    "\n",
    "Here are various tutorials for finetuning BERT: https://drlee.io/fine-tuning-hugging-faces-bert-transformer-for-sentiment-analysis-69b976e6ac5d and https://skimai.com/fine-tuning-bert-for-sentiment-analysis/. I specially recommnend this one: http://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "\n",
    "The dataset where to finetune a BERT related model can be found in the previous cell: **Financial tweets topics dataset** \n",
    "\n",
    "*ALERT*: Running or training a large language model like BERT or FinBERT might incur in large CPU processing times. Although BERT is very large, complicated, and have millions of parameters, we might only need to fine-tune it in only 2-4 epochs. You can also explore Google colab, for limited acces to free GPUs, which might best suited for this task., specially if training required.\n",
    "\n",
    "Finally, compare the previous baseline with fine-tuned FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train['cleaned_sentence'] = cat_train['cleaned_text'].apply(lambda x: ' '.join(x))\n",
    "cat_test['cleaned_sentence'] = cat_test['cleaned_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "sent_train['cleaned_sentence'] = sent_train['cleaned_text'].apply(lambda x: ' '.join(x))\n",
    "sent_test['cleaned_sentence'] = sent_test['cleaned_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Input of the model\n",
    "cat_train_input=cat_train[['cleaned_sentence','label']]\n",
    "cat_test_input=cat_test[['cleaned_sentence','label']]\n",
    "\n",
    "sent_train_input=sent_train[['cleaned_sentence','sentiment']]\n",
    "sent_test_input=sent_test[['cleaned_sentence','sentiment']]\n",
    "\n",
    "#Vectorizing\n",
    "vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     7161\n",
       "positive    3190\n",
       "negative    1473\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_train_input['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapeo de etiquetas a valores numéricos:\n",
      "negative: 0\n",
      "neutral: 1\n",
      "positive: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/13/vtwv0p951k11cf7tvhd750jw0000gn/T/ipykernel_48861/2550375705.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sent_train_input.loc[:,'sentiment_numeric']=label_encoder.fit_transform(sent_train_input['sentiment'])\n",
      "/var/folders/13/vtwv0p951k11cf7tvhd750jw0000gn/T/ipykernel_48861/2550375705.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sent_test_input.loc[:,'sentiment_numeric']=label_encoder.fit_transform(sent_test_input['sentiment'])\n"
     ]
    }
   ],
   "source": [
    "cat_train_tfid = vectorizer.fit_transform(cat_train_input['cleaned_sentence'])\n",
    "y_cat_train=cat_train_input['label']\n",
    "\n",
    "cat_test_tfid = vectorizer.fit_transform(cat_test_input['cleaned_sentence'])\n",
    "y_cat_test=cat_test_input['label']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "sent_train_tfid = vectorizer.fit_transform(sent_train_input['cleaned_sentence'])\n",
    "sent_train_input.loc[:,'sentiment_numeric']=label_encoder.fit_transform(sent_train_input['sentiment'])\n",
    "y_sent_train = sent_train_input['sentiment_numeric']\n",
    "\n",
    "sent_test_tfid = vectorizer.fit_transform(sent_test_input['cleaned_sentence'])\n",
    "sent_test_input.loc[:,'sentiment_numeric']=label_encoder.fit_transform(sent_test_input['sentiment'])\n",
    "y_sent_test= sent_test_input['sentiment_numeric']\n",
    "\n",
    "print(\"Mapeo de etiquetas a valores numéricos:\")\n",
    "for label, numeric_value in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)):\n",
    "    print(f\"{label}: {numeric_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        73\n",
      "           1       0.06      0.04      0.05       214\n",
      "           2       0.21      0.31      0.25       852\n",
      "           3       0.00      0.00      0.00        77\n",
      "           4       0.00      0.00      0.00        97\n",
      "           5       0.02      0.00      0.01       242\n",
      "           6       0.04      0.01      0.02       146\n",
      "           7       0.00      0.00      0.00       160\n",
      "           8       0.00      0.00      0.00        32\n",
      "           9       0.09      0.16      0.12       336\n",
      "          10       0.00      0.00      0.00        13\n",
      "          11       0.00      0.00      0.00        14\n",
      "          12       0.00      0.00      0.00       119\n",
      "          13       0.25      0.09      0.13       116\n",
      "          14       0.24      0.25      0.24       415\n",
      "          15       0.00      0.00      0.00       125\n",
      "          16       0.02      0.02      0.02       249\n",
      "          17       0.33      0.01      0.02       112\n",
      "          18       0.19      0.39      0.25       528\n",
      "          19       0.04      0.02      0.03       197\n",
      "\n",
      "    accuracy                           0.16      4117\n",
      "   macro avg       0.07      0.07      0.06      4117\n",
      "weighted avg       0.12      0.16      0.13      4117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/.local/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/daniel/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/daniel/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/daniel/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(cat_train_tfid, y_cat_train)\n",
    "\n",
    "predictions = model.predict(cat_test_tfid)\n",
    "accuracy = accuracy_score(y_cat_test, predictions)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_cat_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Deployment of the sentiment/category tagger on  financial news or social media posts**\n",
    "\n",
    "Let´s now turn our attention to a live deployment of the financial news tagger. Things can get quite complicated, specially if we add streaming data, so it is best to keep the deploymnet lightweight. There are mainly three important pieces. Let´s explore them:\n",
    "\n",
    "\n",
    "- Build a local dashboard/app (e.g. using Streamlit or another web applications framework of your choice). A bit UI to display the sentiment tagger in action and demonstrate the practical application of your model.\n",
    "\n",
    "\n",
    "- Build a financial news/alerts scraper pipeline, filter some entities if you focus your search. In a real world setting,  you’d likely want to build a more robust infrastructure for processing and ingestion of new examples, handling any preprocessing, and outputting predictions. Here are some options where to scrape data (real-time data might be expensive or limited):\n",
    "\n",
    "    - <span style=\"color:blue\">*Social Media Posts*</span>: Pulling historical or live data from tweets or reddit. There are public APIs with extensive documentation for them.\n",
    "    - <span style=\"color:blue\">*OpenBB*</span>: Open research investment platform. It aggregates financial news across the world and has an API to access them.\n",
    "    - <span style=\"color:blue\">*Financial news outlet*</span>: Yahoo Finance\n",
    "    \n",
    "An pipeline example: The basic premise is to read in a stream of tweets, use a lighweight sentiment analysis engine (BERT might not be a good fit here) to assign a bullish/neutral/bearish score to each tweet, and then see how this cumulatively changes over time.\n",
    "    \n",
    "    \n",
    "- Build an inference endpoint for the tagging model. Within your infrastructure, you can deploy and load the resuting model. One way is to build a REST API endpoint, only to be queried locally (in your laptop).\n",
    "\n",
    "\n",
    "\n",
    "Extra: You could explore or quantify correlations with the market for a list of selected stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
